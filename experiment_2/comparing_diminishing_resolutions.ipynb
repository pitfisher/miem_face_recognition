{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pitfisher/miem_face_recognition/blob/main/FR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_xPg7CuKNTS"
   },
   "outputs": [],
   "source": [
    "!pip install deepface\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kawWam89IW-A"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBeapy7oWlBz",
    "outputId": "29abd27e-33e1-44b1-ec33-ee835c10b87c"
   },
   "outputs": [],
   "source": [
    "input_folder = '/tf/data/upd_face_dataset'\n",
    "\n",
    "resolutions = [1024, 768, 512, 256, 224, 128, 112, 96, 64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"VGG-Face\",\n",
    "    \"Facenet\",\n",
    "    \"Facenet512\",\n",
    "    \"OpenFace\",\n",
    "    \"DeepFace\",\n",
    "    \"DeepID\",\n",
    "    \"ArcFace\",\n",
    "    \"SFace\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "     \"VGG-Face\": 1.17,\n",
    "    \"Facenet\": 0.8,\n",
    "    \"Facenet512\": 1.04,\n",
    "    \"OpenFace\": 0.55,\n",
    "    \"DeepFace\": 0.64,\n",
    "    \"DeepID\": 0.17,\n",
    "    \"ArcFace\": 1.13,\n",
    "    \"SFace\": 1.055,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "iYjb2uBP9oPd"
   },
   "outputs": [],
   "source": [
    "def crop_to_square(image, face_info, border_pixels=0):\n",
    "    # print(f\"Face info: {face_info}\")\n",
    "    x, y, w, h = face_info['facial_area']['x'], face_info['facial_area']['y'], face_info['facial_area']['w'], face_info['facial_area']['h']\n",
    "\n",
    "    # print(f\"x: {x}, y: {y}, w: {w}, h: {h}\")\n",
    "    square_size = max(w, h) + 2 * border_pixels\n",
    "    # new_x = max(0, x + w // 2 - square_size // 2)\n",
    "    # new_y = max(0, y + h // 2 - square_size // 2)\n",
    "\n",
    "    new_x = max(0, x)\n",
    "    new_y = max(0, y)\n",
    "\n",
    "    img = Image.fromarray(image)\n",
    "    cropped_img = img.crop((new_x - border_pixels, new_y - border_pixels, new_x + square_size, new_y + square_size))\n",
    "\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_square_pil(image, face_info, border_pixels=0):\n",
    "    # print(f\"Face info: {face_info}\")\n",
    "    x, y, w, h = face_info['facial_area']['x'], face_info['facial_area']['y'], face_info['facial_area']['w'], face_info['facial_area']['h']\n",
    "\n",
    "    # print(f\"x: {x}, y: {y}, w: {w}, h: {h}\")\n",
    "    square_size = max(w, h) + 2 * border_pixels\n",
    "    # new_x = max(0, x + w // 2 - square_size // 2)\n",
    "    # new_y = max(0, y + h // 2 - square_size // 2)\n",
    "\n",
    "    new_x = max(0, x)\n",
    "    new_y = max(0, y)\n",
    "    cropped_image = image.crop((new_x - (h-w)//2 - border_pixels, new_y - border_pixels, new_x - (h-w)//2 + square_size, new_y + square_size))\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "UyDe3KVlM-he"
   },
   "outputs": [],
   "source": [
    "def resize_image(image, new_resolution):\n",
    "\n",
    "    resized_img = image.resize(new_resolution, Image.LANCZOS)\n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_pil(image, new_resolution):\n",
    "    resized_img = image.resize(new_resolution, Image.LANCZOS)\n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_IyCDVZsYzQe"
   },
   "outputs": [],
   "source": [
    "def save_to_csv(data, columns, filename):\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.to_csv(filename + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "   df_list = []\n",
    "   for resolution in resolutions:\n",
    "      \n",
    "       files = glob.glob(os.path.join(folder_path, \"**\", \"*.JPG\"), recursive=True)\n",
    "    \n",
    "       for img_path in tqdm(files, desc=f'Creating images cropped&resized to {resolution} for {model_name}', unit='image'):\n",
    "        rel_path = os.path.relpath(img_path, folder_path)\n",
    "        rel_path = rel_path.replace(os.path.sep, \"_\")\n",
    "        output_path = os.path.join(tmp_folder, f\"{rel_path[:-4]}_{resolution}.jpg\")\n",
    "      \n",
    "        if not os.path.exists(output_path):\n",
    "            image = Image.open(img_path)\n",
    "            image = ImageOps.exif_transpose(image)\n",
    "            \n",
    "            faces = DeepFace.extract_faces(img_path=img_path, detector_backend=\"retinaface\")\n",
    "            for face_info in faces:\n",
    "                cropped_face = crop_to_square_pil(image, face_info)\n",
    "                resized_img = resize_image_pil(cropped_face, (resolution, resolution))\n",
    "                resized_img.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyplot\n",
    "\n",
    "\n",
    "files = glob.glob(os.path.join(input_folder, \"**\", \"*.JPG\"), recursive=True)\n",
    "\n",
    "img_path = files[44]\n",
    "face_info = DeepFace.extract_faces(img_path=img_path, detector_backend=\"retinaface\")[0]\n",
    "x, y, w, h = face_info['facial_area']['x'], face_info['facial_area']['y'], face_info['facial_area']['w'], face_info['facial_area']['h']\n",
    "\n",
    "image = Image.open(img_path)\n",
    "# https://github.com/python-pillow/Pillow/issues/4703#issuecomment-645219973\n",
    "image = ImageOps.exif_transpose(image)\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "image_array = np.array(image)\n",
    "image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "display(image)\n",
    "\n",
    "cropped_img = crop_to_square(image_array, face_info)\n",
    "img1 = np.array(cropped_img)\n",
    "\n",
    "metrics = []\n",
    "plt.figure()\n",
    "\n",
    "#subplot(r,c) provide the no. of rows and columns\n",
    "f, axarr = plt.subplots(1,2) \n",
    "\n",
    "for resolution in resolutions:\n",
    "    resized_img = resize_image(cropped_img, (resolution, resolution))\n",
    "    img2 = np.array(resized_img)\n",
    "    axarr[0].imshow(img1)\n",
    "    axarr[1].imshow(img2)\n",
    "    plt.show()\n",
    "    result = DeepFace.verify(img1_path=img1,\n",
    "                             img2_path=img2,\n",
    "                             enforce_detection=False,\n",
    "                             detector_backend=\"retinaface\",\n",
    "                             model_name=model_name,\n",
    "                             distance_metric=\"euclidean_l2\"\n",
    "                             )\n",
    "    print(f'distance:{result[\"distance\"]:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2aGBMAE5DusN",
    "outputId": "4e53b2e4-035a-4b5e-c8ea-c35ef188eb6a"
   },
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(input_folder, \"**\", \"*.JPG\"), recursive=True)\n",
    "\n",
    "data = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "\n",
    "    for img_path in tqdm(files, desc=f\"Processing images with {model_name}\", unit=\"image\"):\n",
    "\n",
    "        path = img_path.split(os.path.sep)\n",
    "        filename = f'{path[-2]}_{path[-1]}'\n",
    "        ds_name = f'{model_name}_{path[-4]}_{path[-3]}'\n",
    "\n",
    "        try:\n",
    "            face_info = DeepFace.extract_faces(img_path=img_path, detector_backend=\"retinaface\")[0]\n",
    "        except ValueError as e:\n",
    "            print(f\"Face detection failed {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        image = ImageOps.exif_transpose(image)\n",
    "        image_array = np.array(image)\n",
    "        \n",
    "        cropped_img = crop_to_square_pil(image_array, face_info)\n",
    "        img1 = np.array(cropped_img)\n",
    "\n",
    "        metrics = []\n",
    "        # plt.figure()\n",
    "        \n",
    "        #subplot(r,c) provide the no. of rows and columns\n",
    "        # f, axarr = plt.subplots(1,2) \n",
    " \n",
    "        for resolution in resolutions:\n",
    "\n",
    "            resized_img = resize_image_pil(cropped_img, (resolution, resolution))\n",
    "            img2 = np.array(resized_img)\n",
    "            # axarr[0].imshow(img1)\n",
    "            # axarr[1].imshow(img2)\n",
    "            # plt.show()\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            try:\n",
    "                result = DeepFace.verify(img1_path=img1,\n",
    "                                         img2_path=img2,\n",
    "                                         enforce_detection=False,\n",
    "                                         detector_backend=\"retinaface\",\n",
    "                                         model_name=model_name,\n",
    "                                         distance_metric=\"euclidean_l2\"\n",
    "                                         )\n",
    "                end_time = time.perf_counter()\n",
    "                fin_time = end_time - start_time\n",
    "\n",
    "                metrics.append(f'{result[\"distance\"]:.5f}')\n",
    "                metrics.append(f'{fin_time:.5f}')\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(f\"Verification failed {img_path}: {e}\")\n",
    "                metrics.extend([None, None])\n",
    "\n",
    "        if ds_name not in data:\n",
    "            data[ds_name] = []\n",
    "\n",
    "        data[ds_name].append([filename] + metrics)\n",
    "\n",
    "columns = ['name'] + [f\"{res}_{metric}\" for res in resolutions for metric in [\"distance\", \"time\"]]\n",
    "\n",
    "for ds_name, ds in data.items():\n",
    "    save_to_csv(ds, columns, ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNmgxpYpLKiYp/xsabLSw4K",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
